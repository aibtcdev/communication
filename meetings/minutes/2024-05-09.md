## AIBTCdev Thursday, May 9

### Working Group Updates

Meeting notes updated

- using Claude to summarize for now still
- shared presentation from [CrewAI meetup in SF](https://github.com/stacks-network/stacks/files/15263186/20240425.crewAI.AI.Agents.Meetup.-.Bitcoin.x.AI.pdf) too

New [Twitter account](https://twitter.com/aibtcdev) for AIBTCdev

### Latest AI News

[LLMLingua testing](https://github.com/microsoft/LLMLingua) didn't pan out well

- Luigi saw issues with prompt compression
- idea is good in theory, but failed in practice
- may be irrelevant as context windows increase
- compression worked better with GPT-3.5 level models

The pressure of building AI features [is causing burnout](https://www.cnbc.com/2024/05/03/ai-engineers-face-burnout-as-rat-race-to-stay-competitive-hits-tech.html)

- one issue we discussed was running out of money
- another is trying to keep up with the trends
- quick pivots, lack of testing, breaking changes in releases

More confirmation that [prompts outperform fine tuning](https://the-decoder.com/massive-prompts-outperform-fine-tuning-for-llms-in-new-study-researchers-find/)

- new concept: In-Context Learning (ICL)
- competes with fine-tuning, possibly RAG

LinkedIn published a report on [deploying LLM applications](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product)

- great takeaways [posted by Chip Huyen](https://twitter.com/chipro/status/1787539138562204126)
- YAML over JSON, which has some open debate
  - YAML difficult for other languages?
  - tokenizer has to handle whitespace
- Time to first token and time between token had a big effect on processes
- Automatic evaluation is hard, standardization for annotation
- Initial success misleading, iteration and refinement difficult

[Branching conversations](https://twitter.com/JacobColling/status/1787659433431994392) versus linear UI

- it's like Miro meets AI models
- love the potential here to iterate
- helps define the natural flow better

Quick rundown of new models:

- [Dolphin 2.9.1 Phi-3 Kensho 4.5b](https://twitter.com/erhartford/status/1788059181901283578)
- [Granite Code from IBM](https://twitter.com/_philschmid/status/1787825614570820082) ([paper](https://github.com/ibm-granite/granite-code-models/blob/main/paper.pdf) / [models](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330))
- [DeepSeek V2 (MoE)](https://twitter.com/_philschmid/status/1787494078445195549)
- [Meta-Llama-3-120B-Instruct](https://huggingface.co/mlabonne/Meta-Llama-3-120B-Instruct)
- [Prometheus 2](https://twitter.com/IntuitMachine/status/1788523941704720774): An Open Source Language Model for Evaluating Other Language Models
- Gradient AI: [Llama-3B with 4M context length](https://twitter.com/Gradient_AI_/status/1788258988951589007)

### Development Updates

Exploring different project styles for GitHub

- leaning toward [simple table/board views](https://github.com/orgs/aibtcdev/projects/3)
- same with statuses: to do, in progress, done
- can add additional views and more as needed

https://github.com/orgs/aibtcdev/projects

How we're tackling what's next:

1. making resources available to agents
   - APIs are a good example here
2. making agents available as resources

### Meeting Notes

Key Discussion Points
([failed to generate](https://chat.openai.com/share/0823dbc0-bff9-4d4c-95f9-cb2d3c2a5045) with ChatGPT GPT4o 20240514)
(generated with Claude Opus 20240514)

Key Discussion Points

- Issues with LLMLingua Prompt Compression (Luigi)
  - Prompt compression was horrible, losing important details like names and prices
  - Compression worked better with GPT-3.5 level models
  - May be irrelevant as context windows increase
- Evaluating LLMs and Keeping Up with New Models (Luigi)
  - New models are released frequently, making it difficult to rely on evaluations
  - Prometheus 2 is an LLM trained to evaluate other LLMs, which could be useful
  - Evaluation LLMs must be static to provide reliable results despite the dynamic landscape
- Challenges in Building Resilient AI Systems (Binaya)
  - Need to build systems that are resilient and composable to handle variables like new models
  - Identify deterministic components that don't need to change vs. variables
- Importance of User Perspective in Evaluation (Luigi)
  - Unit tests are important, but capturing user satisfaction is crucial and tricky
  - Need to find ways to automatically capture the quality of results from the user's perspective
- Branching Conversation UI (Jason)
  - Obsidian-based UI allows branching chats vs linear, like Miro meets AI models
  - Enables iterating on parts of a conversation and improving flow
- Hosting Challenges for LLMs (Luigi)
  - Difficult to find good, cost-effective hosting services for LLMs
  - AWS, Google, and others offer solutions but can be expensive compared to using AI APIs
  - Desire to have on-premises deployment, but it's not easy to solve
- Next Steps for AIBTCdev (Jason)
  - Focus on making resources available to agents, such as APIs they can pay for with sBTC
  - Enable agents to provide resources or crews of agents as resources
  - Use GitHub issues to collaborate and make progress between meetings
- Bounty for Agent-Based Game (Patrick)
  - Offering a couple thousand dollar bounty for the best game requiring agents to pay each other
  - Helps kickstart the process and mindset of using sBTC and agents before launch
- Potential for AI IRL Events (Binaya)
  - Recent crypto-focused event in San Francisco was well-received
  - Opportunity to bring together crypto-native builders now immersed in AI
  - San Francisco seems like a prime location for overlap of AI developers and crypto/Bitcoin users
